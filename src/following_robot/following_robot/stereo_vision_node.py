#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŒ‰éœ€åŒç›®ç«‹ä½“è§†è§‰ROS2èŠ‚ç‚¹
=========================
ä¼˜åŒ–ç‰ˆæœ¬ï¼šåªåœ¨è·ç¦»æŸ¥è¯¢æœåŠ¡è¢«è°ƒç”¨æ—¶æ‰è¿›è¡Œç«‹ä½“è§†è§‰å¤„ç†
- å®æ—¶æ˜¾ç¤ºå·¦ç›®å›¾åƒï¼ˆå¸¦å¸§æ•°å’ŒFPSæ˜¾ç¤ºï¼‰
- æŒ‰éœ€æä¾›è·ç¦»æµ‹é‡æœåŠ¡ï¼ˆåªåœ¨æœåŠ¡è°ƒç”¨æ—¶å¤„ç†ç«‹ä½“è§†è§‰ï¼‰
- å¤§å¹…æå‡æ€§èƒ½å’Œé™ä½CPUå ç”¨
- é”®ç›˜äº¤äº’æ§åˆ¶ï¼ˆæŒ‰'q'é€€å‡ºï¼‰
- é›†æˆäººä½“è¡£æœæ£€æµ‹åŠŸèƒ½

ä½œè€…: AI Assistant
ä¼˜åŒ–: æŒ‰éœ€å¤„ç†ç«‹ä½“è§†è§‰ï¼Œæå‡ç³»ç»Ÿæ€§èƒ½
æ–°å¢: äººä½“æ£€æµ‹å’Œèº«ä½“æ¡†å®šåŠŸèƒ½
"""

import rclpy
from rclpy.node import Node
from custom_msgs.srv import GetDistance

import cv2
import numpy as np
import traceback
import threading
import time
from functools import lru_cache

# å¯¼å…¥äººä½“æ£€æµ‹æ¨¡å—
try:
    from .rknn_colour_detect import detect_picture_with_confidence, Determine_the_position_of_the_entire_body
    HUMAN_DETECTION_AVAILABLE = True
    print("âœ… äººä½“æ£€æµ‹æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    HUMAN_DETECTION_AVAILABLE = False
    print(f"âš ï¸ äººä½“æ£€æµ‹æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("ç»§ç»­è¿è¡Œä½†ä¸æä¾›äººä½“æ£€æµ‹åŠŸèƒ½")

# å¯¼å…¥å§¿æ€æ£€æµ‹æ¨¡å—
try:
    from .yolov8_pose_detector import detect_human_pose, draw_human_pose, get_pose_detector
    POSE_DETECTION_AVAILABLE = True
    print("âœ… å§¿æ€æ£€æµ‹æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    POSE_DETECTION_AVAILABLE = False
    print(f"âš ï¸ å§¿æ€æ£€æµ‹æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    print("ç»§ç»­è¿è¡Œä½†ä¸æä¾›å§¿æ€æ£€æµ‹åŠŸèƒ½")


class StereoConfig:
    """ç«‹ä½“è§†è§‰ç³»ç»Ÿé…ç½®ç±»"""
    
    def __init__(self):
        try:
            # ç›¸æœºå†…å‚å’Œå¤–å‚
            self.baseline = 25.100  # åŸºçº¿è·ç¦»
            self.focal_length = 663  # ç„¦è·
            self.cx = 317  # å…‰å¿ƒxåæ ‡
            self.cy = 210  # å…‰å¿ƒyåæ ‡

            # SGBMç®—æ³•å‚æ•° - é’ˆå¯¹æŒ‰éœ€å¤„ç†ä¼˜åŒ–çš„å‚æ•°
            self.minDisparity = 3
            self.numDisparities = 32  # é€‚ä¸­çš„è§†å·®èŒƒå›´ï¼Œå¹³è¡¡ç²¾åº¦å’Œé€Ÿåº¦
            self.blockSize = 7  # é€‚ä¸­çš„å—å¤§å°
            self.P1 = 800
            self.P2 = 3200
            self.disp12MaxDiff = 3
            self.preFilterCap = 31
            self.uniquenessRatio = 10
            self.speckleWindowSize = 100
            self.speckleRange = 32
            self.mode = cv2.STEREO_SGBM_MODE_SGBM_3WAY  # ä½¿ç”¨æ›´ç²¾ç¡®çš„æ¨¡å¼ï¼Œå› ä¸ºä¸éœ€è¦å®æ—¶å¤„ç†

            # ç›¸æœºå‚æ•°
            self.camera_id = 1
            self.frame_width = 1280
            self.frame_height = 480
            self.fps_limit = 60

            # è·ç¦»æµ‹é‡èŒƒå›´
            self.min_distance_mm = 100.0
            self.max_distance_mm = 10000.0
            
        except Exception as e:
            print(f"é…ç½®åˆå§‹åŒ–é”™è¯¯: {e}")
            traceback.print_exc()


class StereoCamera:
    """åŒç›®ç›¸æœºå‚æ•°ç±»"""
    
    def __init__(self):
        try:
            # å·¦ç›¸æœºå†…å‚
            self.cam_matrix_left = np.array([[660.1946, 0, 326.3185], 
                                           [0, 660.8720, 207.1556], 
                                           [0, 0, 1]])

            # å³ç›¸æœºå†…å‚
            self.cam_matrix_right = np.array([[665.1635, 0, 319.9729], 
                                            [0, 665.7919, 212.9630], 
                                            [0, 0, 1]])

            # å·¦å³ç›¸æœºç•¸å˜ç³»æ•°:[k1, k2, p1, p2, k3]
            self.distortion_l = np.array([[-0.0682, 0.1546, 0, 0, 0]])
            self.distortion_r = np.array([[-0.0749, 0.1684, 0, 0, 0]])

            # æ—‹è½¬çŸ©é˜µ
            self.R = np.array([[1.0, 6.140854786327222e-04, -0.0022],
                              [-6.240288417695294e-04, 1, -0.0046],
                              [0.0022, 0.0046, 1]])

            # å¹³ç§»çŸ©é˜µ
            self.T = np.array([[-25.0961], [-0.0869], [-0.1893]])

            # ç„¦è·å’ŒåŸºçº¿è·ç¦»
            self.focal_length = 663
            self.baseline = abs(self.T[0][0])

            # QçŸ©é˜µï¼ˆè§†å·®åˆ°æ·±åº¦çš„æ˜ å°„çŸ©é˜µï¼‰
            self.Q = None  # åœ¨getRectifyTransformä¸­è®¡ç®—
            
        except Exception as e:
            print(f"ç›¸æœºå‚æ•°åˆå§‹åŒ–é”™è¯¯: {e}")
            traceback.print_exc()


class StereoVisionNode(Node):
    """æŒ‰éœ€åŒç›®ç«‹ä½“è§†è§‰ROS2èŠ‚ç‚¹ç±»"""
    
    def __init__(self):
        try:
            super().__init__('stereo_vision_node')
            
            # åŸºæœ¬å±æ€§åˆå§‹åŒ–
            self.config = None
            self.stereo_config = None
            self.cap = None
            self.running = False
            
            # å¸§æ•°æ®ç¼“å­˜ - ç”¨äºæŒ‰éœ€å¤„ç†
            self.current_left_frame = None
            self.current_right_frame = None
            self.frame_lock = threading.Lock()
            self.frame_timestamp = 0
            
            # ç«‹ä½“è§†è§‰ç›¸å…³
            self.map1x = None
            self.map1y = None
            self.map2x = None
            self.map2y = None
            self.Q = None
            
            # ç»Ÿè®¡ä¿¡æ¯
            self.stereo_processing_count = 0
            self.last_stereo_processing_time = 0
            
            # äººä½“æ£€æµ‹ç›¸å…³å±æ€§
            self.human_detection_enabled = HUMAN_DETECTION_AVAILABLE
            self.human_detection_count = 0
            self.last_human_detection_time = 0
            self.current_human_boxes = []  # å½“å‰æ£€æµ‹åˆ°çš„äººä½“æ¡†
            self.current_clothing_detections = []  # å½“å‰æ£€æµ‹åˆ°çš„è¡£æœå’Œè£¤å­è¯¦ç»†ä¿¡æ¯
            
            # å§¿æ€æ£€æµ‹ç›¸å…³å±æ€§
            self.pose_detection_enabled = POSE_DETECTION_AVAILABLE
            self.pose_detection_count = 0
            self.last_pose_detection_time = 0
            self.current_pose_results = []  # å½“å‰æ£€æµ‹åˆ°çš„å§¿æ€ç»“æœ
            
            # åˆå§‹åŒ–é…ç½®
            try:
                self.config = StereoConfig()
                self.get_logger().info('âœ… é…ç½®åˆå§‹åŒ–å®Œæˆ')
            except Exception as e:
                self.get_logger().error(f"âŒ é…ç½®åˆå§‹åŒ–å¤±è´¥: {e}")
                raise
            
            try:
                self.stereo_config = StereoCamera()
                self.get_logger().info('âœ… ç«‹ä½“ç›¸æœºé…ç½®åˆå§‹åŒ–å®Œæˆ')
            except Exception as e:
                self.get_logger().error(f"âŒ ç«‹ä½“ç›¸æœºé…ç½®åˆå§‹åŒ–å¤±è´¥: {e}")
                raise
            
            # é¢„å…ˆè®¡ç®—æ ¡æ­£å˜æ¢çŸ©é˜µ
            try:
                self.setup_stereo_rectification()
                self.get_logger().info('âœ… ç«‹ä½“æ ¡æ­£è®¾ç½®å®Œæˆ')
            except Exception as e:
                self.get_logger().error(f"âŒ ç«‹ä½“æ ¡æ­£è®¾ç½®å¤±è´¥: {e}")
                raise
            
            # åˆå§‹åŒ–ç›¸æœº
            try:
                self.init_camera()
                self.get_logger().info('âœ… ç›¸æœºåˆå§‹åŒ–å®Œæˆ')
            except Exception as e:
                self.get_logger().error(f"âŒ ç›¸æœºåˆå§‹åŒ–å¤±è´¥: {e}")
                pass
            
            # åˆ›å»ºæœåŠ¡
            try:
                self.distance_service = self.create_service(
                    GetDistance,
                    '/stereo/get_distance',
                    self.get_distance_callback
                )
                self.get_logger().info('âœ… ROS2è·ç¦»æµ‹é‡æœåŠ¡åˆ›å»ºå®Œæˆ')
            except Exception as e:
                self.get_logger().error(f"âŒ ROS2æœåŠ¡åˆ›å»ºå¤±è´¥: {e}")
                raise
            
            # å¯åŠ¨å›¾åƒæ•è·çº¿ç¨‹
            if self.cap is not None:
                try:
                    self.start_capture_thread()
                    self.get_logger().info('âœ… å›¾åƒæ•è·çº¿ç¨‹å¯åŠ¨å®Œæˆ')
                except Exception as e:
                    self.get_logger().error(f"âŒ å›¾åƒæ•è·çº¿ç¨‹å¯åŠ¨å¤±è´¥: {e}")
                    pass
            
            self.get_logger().info('âœ… æŒ‰éœ€åŒç›®ç«‹ä½“è§†è§‰èŠ‚ç‚¹åˆå§‹åŒ–å®Œæˆ!')
            self.get_logger().info('ğŸ’¡ ç«‹ä½“è§†è§‰å¤„ç†å°†ä»…åœ¨è·ç¦»æŸ¥è¯¢æœåŠ¡è¢«è°ƒç”¨æ—¶è¿›è¡Œ')
            if HUMAN_DETECTION_AVAILABLE:
                self.get_logger().info('ğŸ¤– äººä½“æ£€æµ‹åŠŸèƒ½å·²å¯ç”¨ï¼ˆæŒ‰"h"é”®åˆ‡æ¢å¼€å…³ï¼‰')
            else:
                self.get_logger().warn('âš ï¸ äººä½“æ£€æµ‹åŠŸèƒ½ä¸å¯ç”¨')
            if POSE_DETECTION_AVAILABLE:
                self.get_logger().info('ğŸ¦´ å§¿æ€æ£€æµ‹åŠŸèƒ½å·²å¯ç”¨ï¼ˆæŒ‰"p"é”®åˆ‡æ¢å¼€å…³ï¼‰')
            else:
                self.get_logger().warn('âš ï¸ å§¿æ€æ£€æµ‹åŠŸèƒ½ä¸å¯ç”¨')
            self.get_logger().info('ğŸ”§ æŒ‰é”®è¯´æ˜: "q"é€€å‡º, "h"åˆ‡æ¢äººä½“æ£€æµ‹, "p"åˆ‡æ¢å§¿æ€æ£€æµ‹, "s"æµ‹è¯•ç«‹ä½“è§†è§‰')
            
        except Exception as e:
            self.get_logger().error(f"âŒ èŠ‚ç‚¹åˆå§‹åŒ–é”™è¯¯: {e}")
            traceback.print_exc()
            self.get_logger().warn("âš ï¸ èŠ‚ç‚¹åœ¨æœ‰é™åŠŸèƒ½ä¸‹å¯åŠ¨")

    def setup_stereo_rectification(self):
        """è®¾ç½®ç«‹ä½“æ ¡æ­£å‚æ•°"""
        try:
            if self.config is None or self.stereo_config is None:
                raise RuntimeError("é…ç½®å¯¹è±¡æœªåˆå§‹åŒ–")
                
            height, width = self.config.frame_height, self.config.frame_width // 2
            result = self.get_rectify_transform(height, width, self.stereo_config)
            
            if result is None or any(item is None for item in result):
                raise RuntimeError("æ ¡æ­£å˜æ¢è®¡ç®—å¤±è´¥")
                
            self.map1x, self.map1y, self.map2x, self.map2y, self.Q = result
            self.get_logger().info('ç«‹ä½“æ ¡æ­£å‚æ•°è®¾ç½®å®Œæˆ')
            
        except Exception as e:
            self.get_logger().error(f"ç«‹ä½“æ ¡æ­£è®¾ç½®é”™è¯¯: {e}")
            traceback.print_exc()
            raise

    def get_rectify_transform(self, height, width, stereo_config):
        """è·å–ç•¸å˜æ ¡æ­£å’Œç«‹ä½“æ ¡æ­£çš„æ˜ å°„å˜æ¢çŸ©é˜µ"""
        try:
            left_K = stereo_config.cam_matrix_left
            right_K = stereo_config.cam_matrix_right
            left_dist = stereo_config.distortion_l
            right_dist = stereo_config.distortion_r
            R = stereo_config.R
            T = stereo_config.T

            # è®¡ç®—ç«‹ä½“æ ¡æ­£å‚æ•°
            R1, R2, P1, P2, Q, roi1, roi2 = cv2.stereoRectify(
                left_K, left_dist, right_K, right_dist,
                (width, height), R, T,
                flags=cv2.CALIB_ZERO_DISPARITY,
                alpha=0.5
            )

            # ä¿å­˜QçŸ©é˜µ
            stereo_config.Q = Q

            # ç”Ÿæˆæ˜ å°„çŸ©é˜µ
            map1x, map1y = cv2.initUndistortRectifyMap(
                left_K, left_dist, R1, P1, (width, height), cv2.CV_32FC1
            )
            map2x, map2y = cv2.initUndistortRectifyMap(
                right_K, right_dist, R2, P2, (width, height), cv2.CV_32FC1
            )

            return map1x, map1y, map2x, map2y, Q
            
        except Exception as e:
            self.get_logger().error(f"æ ¡æ­£å˜æ¢è®¡ç®—é”™è¯¯: {e}")
            traceback.print_exc()
            return None, None, None, None, None

    def init_camera(self):
        """åˆå§‹åŒ–ç›¸æœº"""
        try:
            if self.config is None:
                raise RuntimeError("é…ç½®å¯¹è±¡æœªåˆå§‹åŒ–")
                
            self.cap = cv2.VideoCapture(self.config.camera_id, cv2.CAP_V4L2)
            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.config.frame_width)
            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.config.frame_height)
            # å…¼å®¹ä¸åŒOpenCVç‰ˆæœ¬çš„fourccè®¾ç½®
            try:
                self.cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'))
            except AttributeError:
                # å¦‚æœVideoWriter_fourccä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨æ•°å€¼
                self.cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter.fourcc('M', 'J', 'P', 'G'))
            self.cap.set(cv2.CAP_PROP_CONVERT_RGB, 1)
            
            if not self.cap.isOpened():
                self.get_logger().warn(f"æ— æ³•æ‰“å¼€ç›¸æœºID {self.config.camera_id}ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤ç›¸æœº...")
                self.config.camera_id = 0
                self.cap = cv2.VideoCapture(self.config.camera_id)
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.config.frame_width)
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.config.frame_height)
                self.cap.set(cv2.CAP_PROP_CONVERT_RGB, 1)

                if not self.cap.isOpened():
                    raise RuntimeError("é”™è¯¯ï¼šæ— æ³•æ‰“å¼€ç›¸æœºï¼")
                    
            self.get_logger().info(f'ç›¸æœºåˆå§‹åŒ–æˆåŠŸï¼ŒID: {self.config.camera_id}')
            
        except Exception as e:
            self.get_logger().error(f"ç›¸æœºåˆå§‹åŒ–é”™è¯¯: {e}")
            traceback.print_exc()
            raise

    def start_capture_thread(self):
        """å¯åŠ¨å›¾åƒæ•è·çº¿ç¨‹"""
        try:
            self.running = True
            self.capture_thread = threading.Thread(target=self.capture_loop)
            self.capture_thread.daemon = True
            self.capture_thread.start()
            self.get_logger().info('å›¾åƒæ•è·çº¿ç¨‹å·²å¯åŠ¨')
            
        except Exception as e:
            self.get_logger().error(f"æ•è·çº¿ç¨‹å¯åŠ¨é”™è¯¯: {e}")
            traceback.print_exc()

    def capture_loop(self):
        """å›¾åƒæ•è·å¾ªç¯ - ä»…æ•è·å’Œæ˜¾ç¤ºï¼Œä¸è¿›è¡Œç«‹ä½“å¤„ç†"""
        frame_counter = 0
        start_time = time.time()
        fps_counter = 0
        current_fps = 0.0
        
        while self.running:
            try:
                if self.cap is None:
                    time.sleep(0.1)
                    continue
                    
                ret, frame = self.cap.read()
                if not ret:
                    self.get_logger().warn("æ— æ³•è·å–å›¾åƒ")
                    time.sleep(0.5)
                    continue

                frame_counter += 1
                fps_counter += 1

                # è°ƒæ•´å›¾åƒå¤§å°
                if self.config is not None and (frame.shape[1] != self.config.frame_width or frame.shape[0] != self.config.frame_height):
                    frame = cv2.resize(frame, (self.config.frame_width, self.config.frame_height))

                # åˆ†å‰²å·¦å³å›¾åƒ
                mid_x = frame.shape[1] // 2
                left_half = frame[:, :mid_x]
                right_half = frame[:, mid_x:]

                # ç¼“å­˜å½“å‰å¸§æ•°æ®ä¾›è·ç¦»æœåŠ¡ä½¿ç”¨
                with self.frame_lock:
                    self.current_left_frame = left_half.copy()
                    self.current_right_frame = right_half.copy()
                    self.frame_timestamp = time.time()

                # äººä½“æ£€æµ‹å¤„ç†ï¼ˆæ¯5å¸§æ‰§è¡Œä¸€æ¬¡ä»¥ä¿æŒæ€§èƒ½ï¼‰
                if self.human_detection_enabled and frame_counter % 5 == 0:
                    self.process_human_detection(left_half)
                
                # å§¿æ€æ£€æµ‹å¤„ç†ï¼ˆæ¯8å¸§æ‰§è¡Œä¸€æ¬¡ä»¥ä¿æŒæ€§èƒ½ï¼‰
                if self.pose_detection_enabled and frame_counter % 8 == 0:
                    self.process_pose_detection(left_half)

                # è®¡ç®—å®æ—¶FPS
                current_time = time.time()
                elapsed_time = current_time - start_time
                if elapsed_time >= 1.0:  # æ¯ç§’æ›´æ–°ä¸€æ¬¡FPS
                    current_fps = fps_counter / elapsed_time
                    fps_counter = 0
                    start_time = current_time

                # åˆ›å»ºæ˜¾ç¤ºå›¾åƒ
                display_image = left_half.copy()
                
                # ç»˜åˆ¶å§¿æ€æ£€æµ‹ç»“æœ
                if self.current_pose_results:
                    display_image = draw_human_pose(display_image, self.current_pose_results)
                
                # ç»˜åˆ¶è¡£æœå’Œè£¤å­æ£€æµ‹æ¡†
                if self.current_clothing_detections:
                    for person_clothes in self.current_clothing_detections:
                        person_id = person_clothes['person_id']
                        upper_info = person_clothes['upper']
                        lower_info = person_clothes['lower']
                        
                        # ç»˜åˆ¶ä¸Šè£…æ£€æµ‹æ¡†ï¼ˆè“è‰²ï¼‰
                        if upper_info and len(upper_info) >= 4:
                            x1, y1, x2, y2 = upper_info[:4]
                            cv2.rectangle(display_image, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)
                            # æ·»åŠ ä¸Šè£…æ ‡ç­¾
                            upper_label = f"Upper {person_id+1}"
                            if person_clothes['upper_color']:
                                upper_label += f" {person_clothes['upper_color']}"
                            cv2.putText(display_image, upper_label, (int(x1), int(y1)-10), 
                                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)
                        
                        # ç»˜åˆ¶ä¸‹è£…æ£€æµ‹æ¡†ï¼ˆçº¢è‰²ï¼‰
                        if lower_info and len(lower_info) >= 4:
                            x1, y1, x2, y2 = lower_info[:4]
                            cv2.rectangle(display_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 0, 255), 2)
                            # æ·»åŠ ä¸‹è£…æ ‡ç­¾
                            lower_label = f"Lower {person_id+1}"
                            if person_clothes['lower_color']:
                                lower_label += f" {person_clothes['lower_color']}"
                            cv2.putText(display_image, lower_label, (int(x1), int(y1)-10), 
                                      cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
                
                # ç»˜åˆ¶æ•´ä½“äººä½“æ£€æµ‹æ¡†ï¼ˆç»¿è‰²ï¼‰
                if self.current_human_boxes:
                    for i, box in enumerate(self.current_human_boxes):
                        if len(box) >= 4:
                            x1, y1, x2, y2 = box[:4]
                            cv2.rectangle(display_image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)
                            cv2.putText(display_image, f"Person {i+1}", (int(x1), int(y1)-10), 
                                      cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                
                # æ·»åŠ æ–‡æœ¬ä¿¡æ¯
                font = cv2.FONT_HERSHEY_SIMPLEX
                font_scale = 0.7
                color = (0, 255, 0)  # ç»¿è‰²
                thickness = 2
                
                # æ˜¾ç¤ºå¸§æ•°
                frame_text = f"Frame: {frame_counter}"
                cv2.putText(display_image, frame_text, (10, 30), font, font_scale, color, thickness)
                
                # æ˜¾ç¤ºFPS
                fps_text = f"FPS: {current_fps:.1f}"
                cv2.putText(display_image, fps_text, (10, 60), font, font_scale, color, thickness)
                
                # æ˜¾ç¤ºå›¾åƒåˆ†è¾¨ç‡
                resolution_text = f"Size: {display_image.shape[1]}x{display_image.shape[0]}"
                cv2.putText(display_image, resolution_text, (10, 90), font, font_scale, color, thickness)
                
                # æ˜¾ç¤ºç«‹ä½“å¤„ç†ç»Ÿè®¡ä¿¡æ¯
                stereo_info_text = f"Stereo Calls: {self.stereo_processing_count}"
                cv2.putText(display_image, stereo_info_text, (10, 120), font, font_scale, (255, 255, 0), thickness)
                
                # æ˜¾ç¤ºæœ€åä¸€æ¬¡ç«‹ä½“å¤„ç†æ—¶é—´
                if self.last_stereo_processing_time > 0:
                    time_since_last = current_time - self.last_stereo_processing_time
                    last_process_text = f"Last Stereo: {time_since_last:.1f}s ago"
                    cv2.putText(display_image, last_process_text, (10, 150), font, font_scale, (255, 255, 0), thickness)
                
                # æ˜¾ç¤ºäººä½“æ£€æµ‹ä¿¡æ¯
                if self.human_detection_enabled:
                    human_text = f"Humans: {len(self.current_human_boxes)} detected"
                    cv2.putText(display_image, human_text, (10, 180), font, font_scale, (255, 0, 255), thickness)
                    
                    # æ˜¾ç¤ºè¡£æœæ£€æµ‹è¯¦æƒ…
                    clothing_text = f"Clothes: {len(self.current_clothing_detections)} sets"
                    cv2.putText(display_image, clothing_text, (10, 210), font, font_scale, (255, 0, 255), thickness)
                    
                    detection_count_text = f"Detection Calls: {self.human_detection_count}"
                    cv2.putText(display_image, detection_count_text, (10, 240), font, font_scale, (255, 0, 255), thickness)
                
                # æ˜¾ç¤ºå§¿æ€æ£€æµ‹ä¿¡æ¯
                if self.pose_detection_enabled:
                    pose_text = f"Poses: {len(self.current_pose_results)} detected"
                    cv2.putText(display_image, pose_text, (10, 270), font, font_scale, (0, 255, 255), thickness)
                    
                    pose_count_text = f"Pose Calls: {self.pose_detection_count}"
                    cv2.putText(display_image, pose_count_text, (10, 300), font, font_scale, (0, 255, 255), thickness)
                
                # æ˜¾ç¤ºæ¨¡å¼ä¿¡æ¯
                mode_text = "Mode: ON-DEMAND STEREO + CLOTHING + POSE DETECTION"
                cv2.putText(display_image, mode_text, (10, 330), font, font_scale, (0, 255, 255), thickness)

                # ä½¿ç”¨cv2.imshowæ˜¾ç¤ºå›¾åƒ
                cv2.imshow('Left Camera View (On-Demand Stereo)', display_image)
                
                # å¤„ç†é”®ç›˜äº‹ä»¶
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):  # æŒ‰'q'é€€å‡º
                    self.get_logger().info("ç”¨æˆ·æŒ‰ä¸‹'q'é”®ï¼Œé€€å‡ºç¨‹åº")
                    self.running = False
                    break
                elif key == ord('h'):  # æŒ‰'h'åˆ‡æ¢äººä½“æ£€æµ‹
                    if HUMAN_DETECTION_AVAILABLE:
                        self.human_detection_enabled = not self.human_detection_enabled
                        status = "å¼€å¯" if self.human_detection_enabled else "å…³é—­"
                        self.get_logger().info(f"äººä½“æ£€æµ‹å·²{status}")
                        if not self.human_detection_enabled:
                            self.current_human_boxes = []  # æ¸…ç©ºæ£€æµ‹æ¡†
                            self.current_clothing_detections = []  # æ¸…ç©ºè¡£æœæ£€æµ‹
                    else:
                        self.get_logger().warn("äººä½“æ£€æµ‹æ¨¡å—ä¸å¯ç”¨")
                elif key == ord('p'):  # æŒ‰'p'åˆ‡æ¢å§¿æ€æ£€æµ‹
                    if POSE_DETECTION_AVAILABLE:
                        self.pose_detection_enabled = not self.pose_detection_enabled
                        status = "å¼€å¯" if self.pose_detection_enabled else "å…³é—­"
                        self.get_logger().info(f"å§¿æ€æ£€æµ‹å·²{status}")
                        if not self.pose_detection_enabled:
                            self.current_pose_results = []  # æ¸…ç©ºå§¿æ€æ£€æµ‹ç»“æœ
                    else:
                        self.get_logger().warn("å§¿æ€æ£€æµ‹æ¨¡å—ä¸å¯ç”¨")
                elif key == ord('s'):  # æŒ‰'s'æ‰‹åŠ¨è§¦å‘ç«‹ä½“è§†è§‰å¤„ç†æµ‹è¯•
                    self.get_logger().info("æ‰‹åŠ¨è§¦å‘ç«‹ä½“è§†è§‰å¤„ç†æµ‹è¯•")
                    with self.frame_lock:
                        if self.current_left_frame is not None and self.current_right_frame is not None:
                            test_result = self.process_stereo_on_demand(self.current_left_frame, self.current_right_frame)
                            if test_result is not None:
                                self.get_logger().info("âœ… ç«‹ä½“è§†è§‰å¤„ç†æµ‹è¯•æˆåŠŸ")
                            else:
                                self.get_logger().warn("âŒ ç«‹ä½“è§†è§‰å¤„ç†æµ‹è¯•å¤±è´¥")

                # é«˜å¸§ç‡è¿è¡Œï¼ˆå› ä¸ºæ²¡æœ‰ç«‹ä½“å¤„ç†çš„å¼€é”€ï¼‰
                if self.config is not None:
                        time.sleep(1.0 / self.config.fps_limit)
                else:
                    time.sleep(1.0 / 60)  # é»˜è®¤60fps
                
            except Exception as e:
                self.get_logger().error(f"å›¾åƒæ•è·å¾ªç¯é”™è¯¯: {e}")
                traceback.print_exc()
                time.sleep(0.01)

    def process_human_detection(self, image):
        """å¤„ç†äººä½“æ£€æµ‹"""
        try:
            if not self.human_detection_enabled:
                return
            
            self.get_logger().debug("ğŸ¤– å¼€å§‹äººä½“æ£€æµ‹...")
            detection_start_time = time.time()
            
            # è°ƒç”¨äººä½“æ£€æµ‹å‡½æ•°
            detection_results = detect_picture_with_confidence(image)
            
            if detection_results:
                # æå–äººä½“ä½ç½®å’Œè¡£æœè¯¦ç»†ä¿¡æ¯
                human_boxes = []
                clothing_detections = []
                
                for person_idx, result in enumerate(detection_results):
                    if len(result) >= 2:  # ç¡®ä¿æœ‰ä¸Šè£…å’Œä¸‹è£…ä¿¡æ¯
                        upper_info = result[0] if result[0] else None
                        lower_info = result[1] if result[1] else None
                        
                        # ä¿å­˜è¡£æœå’Œè£¤å­çš„è¯¦ç»†æ£€æµ‹ä¿¡æ¯
                        person_clothes = {
                            'person_id': person_idx,
                            'upper': upper_info,
                            'lower': lower_info,
                            'upper_color': result[2] if len(result) > 2 else None,
                            'lower_color': result[3] if len(result) > 3 else None
                        }
                        clothing_detections.append(person_clothes)
                        
                        # ä½¿ç”¨Determine_the_position_of_the_entire_bodyå‡½æ•°è·å–æ•´ä½“æ¡†
                        try:
                            body_box = Determine_the_position_of_the_entire_body(
                                upper_info, lower_info, image
                            )
                            if body_box:
                                human_boxes.append(body_box)
                        except Exception as e:
                            self.get_logger().debug(f"èº«ä½“æ¡†è®¡ç®—å¤±è´¥: {e}")
                
                # æ›´æ–°æ£€æµ‹ç»“æœ
                self.current_human_boxes = human_boxes
                self.current_clothing_detections = clothing_detections
                self.human_detection_count += 1
                self.last_human_detection_time = time.time()
                
                detection_time = (self.last_human_detection_time - detection_start_time) * 1000
                self.get_logger().debug(f"âœ… æ£€æµ‹åˆ° {len(human_boxes)} ä¸ªäººä½“ï¼Œ{len(clothing_detections)} å¥—è¡£æœï¼Œè€—æ—¶: {detection_time:.2f}ms")
            else:
                self.current_human_boxes = []
                self.current_clothing_detections = []
                
        except Exception as e:
            self.get_logger().error(f"äººä½“æ£€æµ‹å¤„ç†é”™è¯¯: {e}")
            traceback.print_exc()
            self.current_human_boxes = []
            self.current_clothing_detections = []

    def process_pose_detection(self, image):
        """å¤„ç†å§¿æ€æ£€æµ‹"""
        try:
            if not self.pose_detection_enabled:
                return
            
            self.get_logger().debug("ğŸ¤– å¼€å§‹å§¿æ€æ£€æµ‹...")
            detection_start_time = time.time()
            
            # è°ƒç”¨å§¿æ€æ£€æµ‹å‡½æ•°
            pose_results = detect_human_pose(image)
            
            if pose_results:
                # æ›´æ–°æ£€æµ‹ç»“æœ
                self.current_pose_results = pose_results
                self.pose_detection_count += 1
                self.last_pose_detection_time = time.time()
                
                detection_time = (self.last_pose_detection_time - detection_start_time) * 1000
                self.get_logger().debug(f"âœ… æ£€æµ‹åˆ° {len(pose_results)} ä¸ªäººä½“å§¿æ€ï¼Œè€—æ—¶: {detection_time:.2f}ms")
            else:
                self.current_pose_results = []
                
        except Exception as e:
            self.get_logger().error(f"å§¿æ€æ£€æµ‹å¤„ç†é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            self.current_pose_results = []

    def process_stereo_on_demand(self, left_image, right_image):
        """æŒ‰éœ€å¤„ç†ç«‹ä½“è§†è§‰ - ä»…åœ¨æœåŠ¡è°ƒç”¨æ—¶æ‰§è¡Œ"""
        try:
            self.get_logger().info("ğŸ”„ å¼€å§‹æŒ‰éœ€ç«‹ä½“è§†è§‰å¤„ç†...")
            process_start_time = time.time()
            
            # æ£€æŸ¥å¿…è¦çš„å±æ€§æ˜¯å¦å­˜åœ¨
            if not hasattr(self, 'stereo_config') or self.stereo_config is None:
                self.get_logger().error("âŒ stereo_configæœªåˆå§‹åŒ–")
                return None
                
            if not hasattr(self, 'map1x') or self.map1x is None:
                self.get_logger().error("âŒ æ ¡æ­£æ˜ å°„çŸ©é˜µæœªåˆå§‹åŒ–")
                return None
                
            if not hasattr(self, 'Q') or self.Q is None:
                self.get_logger().error("âŒ é‡æŠ•å½±çŸ©é˜µQæœªåˆå§‹åŒ–")
                return None
            
            # æ¶ˆé™¤ç•¸å˜
            iml = self.undistortion(left_image, self.stereo_config.cam_matrix_left, 
                                  self.stereo_config.distortion_l)
            imr = self.undistortion(right_image, self.stereo_config.cam_matrix_right, 
                                  self.stereo_config.distortion_r)

            # é¢„å¤„ç†å›¾åƒ
            iml_processed, imr_processed = self.preprocess(iml, imr)

            # å›¾åƒæ ¡æ­£
            iml_rectified, imr_rectified = self.rectify_image(
                iml_processed, imr_processed, 
                self.map1x, self.map1y, self.map2x, self.map2y
            )

            # è®¡ç®—è§†å·®å›¾
            disparity = self.stereo_match_sgbm(iml_rectified, imr_rectified)

            # è®¡ç®—3Dç‚¹äº‘
            points_3d = self.reproject_to_3d(disparity, self.Q)

            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.stereo_processing_count += 1
            self.last_stereo_processing_time = time.time()
            
            process_time = self.last_stereo_processing_time - process_start_time
            self.get_logger().info(f"âœ… ç«‹ä½“è§†è§‰å¤„ç†å®Œæˆï¼Œè€—æ—¶: {process_time:.3f}ç§’")

            return points_3d
            
        except Exception as e:
            self.get_logger().error(f"æŒ‰éœ€ç«‹ä½“è§†è§‰å¤„ç†é”™è¯¯: {e}")
            traceback.print_exc()
            return None

    def undistortion(self, image, camera_matrix, dist_coeff):
        """æ¶ˆé™¤å›¾åƒç•¸å˜"""
        try:
            h, w = image.shape[:2]
            new_camera_matrix, roi = cv2.getOptimalNewCameraMatrix(
                camera_matrix, dist_coeff, (w, h), 1, (w, h)
            )
            undistorted = cv2.undistort(image, camera_matrix, dist_coeff, None, new_camera_matrix)

            x, y, w, h = roi
            if w > 0 and h > 0:
                undistorted = undistorted[y:y + h, x:x + w]

            return undistorted
            
        except Exception as e:
            self.get_logger().error(f"ç•¸å˜æ ¡æ­£é”™è¯¯: {e}")
            traceback.print_exc()
            return image

    def preprocess(self, img1, img2):
        """å›¾åƒé¢„å¤„ç†"""
        try:
            # è½¬æ¢ä¸ºç°åº¦å›¾
            if img1.ndim == 3:
                img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
            else:
                img1_gray = img1.copy()

            if img2.ndim == 3:
                img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
            else:
                img2_gray = img2.copy()

            # åº”ç”¨CLAHEå¢å¼ºå¯¹æ¯”åº¦
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
            img1_eq = clahe.apply(img1_gray)
            img2_eq = clahe.apply(img2_gray)

            return img1_eq, img2_eq
            
        except Exception as e:
            self.get_logger().error(f"å›¾åƒé¢„å¤„ç†é”™è¯¯: {e}")
            traceback.print_exc()
            return img1, img2

    def rectify_image(self, image1, image2, map1x, map1y, map2x, map2y):
        """å¯¹å›¾åƒåº”ç”¨ç•¸å˜æ ¡æ­£å’Œç«‹ä½“æ ¡æ­£"""
        try:
            rectified_img1 = cv2.remap(image1, map1x, map1y, cv2.INTER_AREA)
            rectified_img2 = cv2.remap(image2, map2x, map2y, cv2.INTER_AREA)
            return rectified_img1, rectified_img2
            
        except Exception as e:
            self.get_logger().error(f"å›¾åƒæ ¡æ­£é”™è¯¯: {e}")
            traceback.print_exc()
            return image1, image2

    def stereo_match_sgbm(self, left_image, right_image):
        """ä½¿ç”¨SGBMç®—æ³•è®¡ç®—è§†å·®å›¾"""
        try:
            # ç¡®ä¿è¾“å…¥å›¾åƒæ˜¯ç°åº¦å›¾
            if left_image.ndim != 2:
                left_image = cv2.cvtColor(left_image, cv2.COLOR_BGR2GRAY)
            if right_image.ndim != 2:
                right_image = cv2.cvtColor(right_image, cv2.COLOR_BGR2GRAY)

            # æ£€æŸ¥é…ç½®æ˜¯å¦å­˜åœ¨
            if self.config is None:
                self.get_logger().error("âŒ configæœªåˆå§‹åŒ–")
                return None
                
            # åˆ›å»ºSGBMåŒ¹é…å™¨ï¼ˆå…¼å®¹ä¸åŒOpenCVç‰ˆæœ¬ï¼‰
            try:
                left_matcher = cv2.StereoSGBM_create(
                    minDisparity=self.config.minDisparity,
                    numDisparities=self.config.numDisparities,
                    blockSize=self.config.blockSize,
                    P1=self.config.P1,
                    P2=self.config.P2,
                    disp12MaxDiff=self.config.disp12MaxDiff,
                    preFilterCap=self.config.preFilterCap,
                    uniquenessRatio=self.config.uniquenessRatio,
                    speckleWindowSize=self.config.speckleWindowSize,
                    speckleRange=self.config.speckleRange,
                    mode=self.config.mode
                )
            except AttributeError:
                # å¦‚æœStereoSGBM_createä¸å¯ç”¨ï¼Œä½¿ç”¨æ—§ç‰ˆAPI
                left_matcher = cv2.createStereoSGBM(
                    minDisparity=self.config.minDisparity,
                    numDisparities=self.config.numDisparities,
                    blockSize=self.config.blockSize,
                    P1=self.config.P1,
                    P2=self.config.P2,
                    disp12MaxDiff=self.config.disp12MaxDiff,
                    preFilterCap=self.config.preFilterCap,
                    uniquenessRatio=self.config.uniquenessRatio,
                    speckleWindowSize=self.config.speckleWindowSize,
                    speckleRange=self.config.speckleRange,
                    mode=self.config.mode
                )

            # è®¡ç®—è§†å·®å›¾
            disparity = left_matcher.compute(left_image, right_image)
            disparity = disparity.astype(np.float32) / 16.0

            # è¿‡æ»¤å°è§†å·®å€¼
            min_valid_disp = 1.0
            disparity[disparity < min_valid_disp] = 0

            return disparity
            
        except Exception as e:
            self.get_logger().error(f"ç«‹ä½“åŒ¹é…é”™è¯¯: {e}")
            traceback.print_exc()
            return None

    def reproject_to_3d(self, disparity, Q):
        """å°†è§†å·®å›¾è½¬æ¢ä¸º3Dç‚¹äº‘"""
        try:
            if disparity is None or Q is None:
                return None
                
            # è¿‡æ»¤å¤ªå°çš„è§†å·®å€¼
            filtered_disp = disparity.copy()
            min_disparity = 1.0
            filtered_disp[filtered_disp < min_disparity] = 0

            # ä½¿ç”¨OpenCVçš„reprojectImageTo3Dè¿›è¡Œé‡æŠ•å½±
            points_3d = cv2.reprojectImageTo3D(filtered_disp, Q)

            # è¿‡æ»¤æ·±åº¦å€¼å¼‚å¸¸çš„ç‚¹
            max_depth = self.config.max_distance_mm if self.config else 10000.0
            mask = (points_3d[:, :, 2] > 0) & (points_3d[:, :, 2] < max_depth)
            points_3d[~mask] = [0, 0, 0]

            return points_3d
            
        except Exception as e:
            self.get_logger().error(f"3Dé‡æŠ•å½±é”™è¯¯: {e}")
            traceback.print_exc()
            return None

    def get_distance_callback(self, request, response):
        """è·ç¦»æµ‹é‡æœåŠ¡å›è°ƒå‡½æ•° - æŒ‰éœ€è¿›è¡Œç«‹ä½“è§†è§‰å¤„ç†"""
        try:
            self.get_logger().info(f"ğŸ“ æ”¶åˆ°è·ç¦»æŸ¥è¯¢è¯·æ±‚: åæ ‡({request.x}, {request.y})")
            
            # è·å–å½“å‰å¸§æ•°æ®
            with self.frame_lock:
                if self.current_left_frame is None or self.current_right_frame is None:
                    response.success = False
                    response.distance = 0.0
                    response.message = "å½“å‰æ— å¯ç”¨å›¾åƒæ•°æ®"
                    self.get_logger().warn("âŒ æ— å¯ç”¨å›¾åƒæ•°æ®")
                    return response
                
                # å¤åˆ¶å½“å‰å¸§æ•°æ®
                left_frame = self.current_left_frame.copy()
                right_frame = self.current_right_frame.copy()
                frame_age = time.time() - self.frame_timestamp

            self.get_logger().info(f"ğŸ“¸ ä½¿ç”¨å›¾åƒæ•°æ®ï¼Œå¹´é¾„: {frame_age:.3f}ç§’")
            
            # æŒ‰éœ€è¿›è¡Œç«‹ä½“è§†è§‰å¤„ç†
            points_3d = self.process_stereo_on_demand(left_frame, right_frame)

            if points_3d is None:
                response.success = False
                response.distance = 0.0
                response.message = "ç«‹ä½“è§†è§‰å¤„ç†å¤±è´¥"
                self.get_logger().error("âŒ ç«‹ä½“è§†è§‰å¤„ç†å¤±è´¥")
                return response

            # æµ‹é‡æŒ‡å®šç‚¹çš„è·ç¦»
            distance = self.measure_distance(points_3d, request.x, request.y)

            if distance is not None:
                response.success = True
                response.distance = distance
                response.message = f"æˆåŠŸæµ‹é‡è·ç¦»: {distance:.2f}ç±³"
                self.get_logger().info(f"âœ… æµ‹é‡ç‚¹({request.x}, {request.y})è·ç¦»: {distance:.2f}ç±³")
            else:
                response.success = False
                response.distance = 0.0
                response.message = "æ— æ³•æµ‹é‡è¯¥ç‚¹è·ç¦»ï¼Œå¯èƒ½æ˜¯æ— æ•ˆç‚¹"
                self.get_logger().warn(f"âš ï¸ æ— æ³•æµ‹é‡ç‚¹({request.x}, {request.y})çš„è·ç¦»")

            return response
            
        except Exception as e:
            self.get_logger().error(f"è·ç¦»æœåŠ¡å›è°ƒé”™è¯¯: {e}")
            traceback.print_exc()
            response.success = False
            response.distance = 0.0
            response.message = f"æœåŠ¡å¤„ç†é”™è¯¯: {str(e)}"
            return response

    def measure_distance(self, points_3d, x, y):
        """æµ‹é‡æŒ‡å®šåƒç´ ç‚¹åˆ°ç›¸æœºçš„è·ç¦»"""
        try:
            h, w = points_3d.shape[:2]

            # æ£€æŸ¥åæ ‡æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
            if not (0 <= x < w and 0 <= y < h):
                self.get_logger().warn(f"åæ ‡({x}, {y})è¶…å‡ºèŒƒå›´({w}x{h})")
                return None

            # è·å–ç‚¹çš„3Dåæ ‡
            point_3d = points_3d[y, x]

            # æ£€æŸ¥ç‚¹çš„æœ‰æ•ˆæ€§
            if np.all(np.isfinite(point_3d)) and not np.all(point_3d == 0):
                # è®¡ç®—æ¬§å‡ é‡Œå¾—è·ç¦»
                distance = np.sqrt(np.sum(point_3d ** 2))
                return distance / 1000.0  # è½¬æ¢ä¸ºç±³

            return None
            
        except Exception as e:
            self.get_logger().error(f"è·ç¦»æµ‹é‡é”™è¯¯: {e}")
            traceback.print_exc()
            return None

    def destroy_node(self):
        """èŠ‚ç‚¹é”€æ¯æ—¶çš„æ¸…ç†å·¥ä½œ"""
        try:
            self.running = False
            if hasattr(self, 'cap') and self.cap is not None:
                self.cap.release()
            cv2.destroyAllWindows()
            
            self.get_logger().info(f'ğŸ“Š ç«‹ä½“è§†è§‰å¤„ç†ç»Ÿè®¡: æ€»å…±å¤„ç†äº† {self.stereo_processing_count} æ¬¡')
            self.get_logger().info('âœ… æŒ‰éœ€åŒç›®ç«‹ä½“è§†è§‰èŠ‚ç‚¹å·²å…³é—­')
            super().destroy_node()
            
        except Exception as e:
            self.get_logger().error(f"èŠ‚ç‚¹é”€æ¯é”™è¯¯: {e}")
            traceback.print_exc()


def main(args=None):
    """ä¸»å‡½æ•°"""
    try:
        rclpy.init(args=args)
        node = StereoVisionNode()
        
        try:
            rclpy.spin(node)
        except KeyboardInterrupt:
            pass
        finally:
            node.destroy_node()
            rclpy.shutdown()
            
    except Exception as e:
        print(f"ä¸»å‡½æ•°é”™è¯¯: {e}")
        traceback.print_exc()


if __name__ == '__main__':
    main() 